1. #SparkSession: The entry point to programming Spark with the DataFrame and SQL API.
#sum: An aggregation function to calculate the sum of a numerical column.
#col: A function to reference a column in a DataFrame.

#import pyspark module 
# Import necessary packages for Spark session

from pyspark.sql import SparkSession

# Import functions for DataFrame operations
from pyspark.sql import functions as F


# Import types for defining schemas
from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, TimestampType


# Create a Spark session
spark = SparkSession.builder \
        .appName("VerifyPySparkInstallation") \
        .getOrCreate()


# Load a CSV file into a DataFrame
df = spark.read.csv("/Users/jeykanesh/Desktop/coding/data.csv", header=True, inferSchema=True)
df.show()


# Register the DataFrame as a temporary table
df.createOrReplaceTempView("my_table")

# Run a SQL query
sql_df = spark.sql("SELECT user_id, COUNT(*) FROM my_table GROUP BY user_id")
sql_df.show()

