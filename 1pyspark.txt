
1. First, you need to install PySpark using pip. Hereâ€™s the command to do that:


pip install pyspark


2. You can try importing PySpark in a Python script or a Jupyter notebook. If there are no import errors, the installation was successful.

try:
    import pyspark
    print("PySpark is installed correctly.")
except ImportError as e:
    print("PySpark is not installed.")



You can list the installed packages to see if PySpark is among them.

pip list


3. Install python packages:


# Import necessary packages for Spark session
from pyspark.sql import SparkSession

# Import functions for DataFrame operations
from pyspark.sql import functions as F


from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType, DoubleType, TimestampType



4. # Run a simple command

df = spark.createDataFrame([(1, "one"), (2, "two"), (3, "three")], ["number", "word"])
df.show()



5.Loading Data: Load data from various sources such as CSV, JSON, Parquet, etc.

# Load a CSV file into a DataFrame
df = spark.read.csv("path/to/your/file.csv", header=True, inferSchema=True)
df.show()


6.

#DataFrame Operations: Perform operations like filtering, selecting, grouping, and aggregating.

# Select specific columns
df_selected = df.select("user_id", "transaction_amount")

# Filter rows
df_filtered = df.filter(df["user_id"] > 10)

# Group by and aggregate
df_grouped = df.groupBy("user_id").agg(F.sum("transaction_amount").alias("sum_transaction_amount"))
df_grouped.show()


7.
SQL Queries: Register DataFrame as a temporary table and run SQL queries.

# Register the DataFrame as a temporary table
df.createOrReplaceTempView("my_table")

# Run a SQL query
sql_df = spark.sql("SELECT user_id, COUNT(*) FROM my_table GROUP BY user_id")
sql_df.show()











