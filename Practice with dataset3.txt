Instructions:
1.Imports: Ensure all necessary PySpark modules are imported.
2.Spark Session: Create a Spark session before performing any operations.
3.Schema Definition: Define the schema for your DataFrame.
4.Load Data: Load the CSV file into a DataFrame using the defined schema.
5.Transformations: Perform various transformations on the DataFrame.
6.Actions: Execute actions to trigger the computation and retrieve results.






1.#Packages:

#Import Packages

from pyspark.sql import SparkSession
from pyspark.sql.types import StructType, StructField, IntegerType, DoubleType
from pyspark.sql import functions as F

2. 

# Create a Spark session
spark = SparkSession.builder \
    .appName("TransformationsAndActions") \
    .getOrCreate()


3. 


# Define schema for the data
schema = StructType([
    StructField("user_id", IntegerType(), True),
    StructField("transaction_amount", DoubleType(), True)
])


4. 

# Load the CSV file into a DataFrame
file_path = '/Users/Jeykanesh/Desktop/coding/data.csv'
df = spark.read.csv(file_path, schema=schema, header=True)


5.

# Show the DataFrame
print("Original DataFrame:")
df.show()

6.

# Transformations
print("Selected Columns:")
df_selected = df.select("user_id", "transaction_amount")
df_selected.show()

7.

print("Filtered Rows (transaction_amount > 500):")
df_filtered = df.filter(df["transaction_amount"] > 500)
df_filtered.show()

8.

print("New Column with Tax (transaction_amount * 1.1):")
df_with_new_column = df.withColumn("transaction_with_tax", df["transaction_amount"] * 1.1)
df_with_new_column.show()

9.

print("Grouped by user_id and Aggregated Sum of transaction_amount:")
df_grouped = df.groupBy("user_id").agg(F.sum("transaction_amount").alias("total_amount"))
df_grouped.show()

10.

print("Sorted by transaction_amount Descending:")
df_sorted = df.orderBy("transaction_amount", ascending=False)
df_sorted.show()

11.

# Actions
print("Collected Data:")
collected_data = df.collect()
print(collected_data)

12.

print(f"Row Count: {df.count()}")

13.

print("First Two Rows:")
first_two_rows = df.take(2)
print(first_two_rows)

14.

print("Describe the DataFrame:")
df.describe().show()

15.

print("First Row:")
first_row = df.first()
print(first_row)

16.

print("Schema:")
df.printSchema()

17.
# Stop the Spark session
spark.stop()

